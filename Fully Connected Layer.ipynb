{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fully Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, confusion_matrix ,  accuracy_score\n",
    "from sklearn import datasets \n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Toy data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 2) (6, 2)\n"
     ]
    }
   ],
   "source": [
    "X=np.array([[1, 2], [4, 5], [12, 7], [16, 10], [20, 5], [5, 8]  ])\n",
    "y=np.array([[0,1], [0,1], [1,0], [1,0],  [0,1], [1,0] ])\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Softmax</b>\n",
    "$$Let, a=[a_0, a_1, a_2, ..]$$\n",
    "$$S_i=\\frac{e^{a_i}}{\\sum_{k=0}^{N} {e^{a_k}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>First Task : Implement Softmax</b> <br>\n",
    "1. Implement both ways: Using loop and Without using loop. <br>\n",
    "2. What challanges did you face?\n",
    "3. Check output manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2 1.6]\n",
      " [2.4 3.4]\n",
      " [3.8 5.8]\n",
      " [5.1 7.8]\n",
      " [4.  6.6]\n",
      " [3.4 4.8]]\n"
     ]
    }
   ],
   "source": [
    "z=np.array( [ [1.2, 1.6], [2.4, 3.4],  [3.8, 5.8],  [5.1, 7.8],  [4. , 6.6],   [3.4, 4.8]])\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40131234 0.59868766]\n",
      " [0.26894142 0.73105858]\n",
      " [0.11920292 0.88079708]\n",
      " [0.06297336 0.93702664]\n",
      " [0.06913842 0.93086158]\n",
      " [0.19781611 0.80218389]]\n"
     ]
    }
   ],
   "source": [
    "#a=softmax(z)\n",
    "\n",
    "# Using Loop\n",
    "\n",
    "L=[]\n",
    "def sign(a):\n",
    "    d1=math.exp(a[0])\n",
    "    d2=math.exp(a[1])\n",
    "    d=d1+d2\n",
    "    return [math.exp(a[0])/d,math.exp(a[1])/d];\n",
    "\n",
    "\n",
    "for i in z:\n",
    "    L.append(sign(i))\n",
    "\n",
    "#print(L)\n",
    "L=np.array(L)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4013123398875481 0.5986876601124522 \n",
      "0.26894142136999505 0.7310585786300048 \n",
      "0.11920292202211759 0.8807970779778826 \n",
      "0.06297335605699647 0.9370266439430033 \n",
      "0.06913842034334684 0.9308615796566532 \n",
      "0.19781611144141834 0.802183888558582 \n"
     ]
    }
   ],
   "source": [
    "from scipy.special import softmax\n",
    "soft=softmax(z,axis=1)\n",
    "\n",
    "#print(aa)\n",
    "\n",
    "for i in soft:\n",
    "    for j in i:\n",
    "        print(j,end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Second Task : calculate feedforward output</b><br><br>\n",
    "$A=softmax(X.w+b)$\n",
    "\n",
    "Output should look like the following. <br>\n",
    "\n",
    "[[0.401 0.599] <br>\n",
    " [0.269 0.731] <br>\n",
    " [0.119 0.881] <br>\n",
    " [0.063 0.937] <br>\n",
    " [0.069 0.931] <br>\n",
    " [0.198 0.802]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.array([0.5, 0.6])\n",
    "w=np.array([ [0.1,0.2], [0.3,0.4] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.2 1.6]\n",
      " [2.4 3.4]\n",
      " [3.8 5.8]\n",
      " [5.1 7.8]\n",
      " [4.  6.6]\n",
      " [3.4 4.8]]\n",
      "[[0.40131234 0.59868766]\n",
      " [0.26894142 0.73105858]\n",
      " [0.11920292 0.88079708]\n",
      " [0.06297336 0.93702664]\n",
      " [0.06913842 0.93086158]\n",
      " [0.19781611 0.80218389]]\n"
     ]
    }
   ],
   "source": [
    "z=np.dot(X,w)+b\n",
    "print(z)\n",
    "\n",
    "A=softmax(z,axis=1)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Third Task : calculate log loss</b><br><br> \n",
    "Note: <b>without using any loop.</b> <br>\n",
    "$$L_i = - y_i* \\sum \\log (A_i)$$\n",
    "\\begin{equation*}\n",
    "L =\\frac{1}{N} \\sum_i L_i \n",
    "\\end{equation*}\n",
    "\n",
    "Output should look like the following. <br>\n",
    "Loss: [0.513 0.313 2.127 2.765 0.072 1.62 ] <br>\n",
    "average loss: 1.2351"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss\n",
      "  [[0.51301525]\n",
      " [0.31326169]\n",
      " [2.12692801]\n",
      " [2.76504356]\n",
      " [0.07164469]\n",
      " [1.62041741]]\n",
      "\n",
      "Average Loss : 0.5\n"
     ]
    }
   ],
   "source": [
    "Loss= np.sum(-y*np.log(A),axis=1, keepdims=True)\n",
    "print(\"Loss\\n \",Loss)\n",
    "print()\n",
    "print(\"Average Loss :\", np.average(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fourth Task : calculate gradient</b><br>\n",
    "Note: <b>calculate dz, dw, db without using loop</b> </br>\n",
    "$$dz=A-y$$ \n",
    "$$dw=\\frac{1}{N} ( X^T.dz )$$   \n",
    "$$db=\\frac{1}{N} \\sum dz $$ \n",
    "\n",
    "Dimension check: y-yh= (2xN).(NX2)=(2x2) \n",
    "\n",
    "dw should look like below, <br>\n",
    "[[-4.452,  4.452], <br>\n",
    "[-3.243,  3.243]])\n",
    "\n",
    "db should look like below<br>\n",
    "[[-0.313,  0.313]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dz : \n",
      " [[ 0.40131234 -0.40131234]\n",
      " [ 0.26894142 -0.26894142]\n",
      " [-0.88079708  0.88079708]\n",
      " [-0.93702664  0.93702664]\n",
      " [ 0.06913842 -0.06913842]\n",
      " [-0.80218389  0.80218389]]\n",
      "Dw : \n",
      " [[-4.45217737  4.45217737]\n",
      " [-3.2433822   3.2433822 ]]\n",
      "Db : \n",
      " [-0.3134359  0.3134359]\n"
     ]
    }
   ],
   "source": [
    "dz=A-y\n",
    "print(\"Dz : \\n\",dz)\n",
    "\n",
    "x=X.transpose()\n",
    "dw=np.dot(x,dz)/len(dz)\n",
    "\n",
    "print('Dw : \\n',dw)\n",
    "\n",
    "db=sum(dz)/len(dz)\n",
    "\n",
    "print('Db : \\n',db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Fifth Task : Train the network using gradient descent optimization</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.01\n",
    "epoch=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init w and b randomly.\n",
    "D=X.shape[1]\n",
    "C=y.shape[1]\n",
    "w = 0.01 * np.random.randn(D,C)\n",
    "b = np.zeros((1,C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. Init w and b randomly\n",
    " 2. repeat the following\n",
    "     3. calculate feedforward output\n",
    "     4. calculate gradient\n",
    "     5. update w and b (w=w-lr\\*dw,  b=b-lr*db)\n",
    "     6. calculate loss and keep track.\n",
    "\n",
    "7. After training plot the loss\n",
    "8. test prediction on the X data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Digits Data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas=datasets.load_digits()\n",
    "data=datas['data']\n",
    "label=datas['target'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shape: (1437, 64) (1437,)\n",
      "Testing shape: (360, 64) (360,)\n"
     ]
    }
   ],
   "source": [
    "X,X_test, y, y_test=train_test_split(data, label, test_size = 0.2)\n",
    "print('Training shape:', X.shape, y.shape)\n",
    "print('Testing shape:',X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N,D=X.shape            #num_data, data_dimension\n",
    "C=np.max(y)+1          #number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y\n",
    "y_onehot=np.zeros((len(y), C))\n",
    "y_onehot[ range(len(y)), y]=1\n",
    "print(y_onehot)\n",
    "y=np.copy(y_onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Sixth Task : Train on the digits data [Note: y is not one-hot vector now]</b>\n",
    "1. Train a two layer[Input, Output] network\n",
    "2. Plot loss, tune learning rate, number of epochs\n",
    "3. predict on test data and show accuracy.  [accuracy should be above 90%]\n",
    "4. Play with the learning rate and number of epochs [try few values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
